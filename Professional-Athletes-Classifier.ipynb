{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Professional Athlete Classifier Using Azure Machine Learning Service \n",
    "\n",
    "The following tutorial demonstrates an end-to-end workflow for training and deploying a **Tensorflow 2.0** model on Azure Machine Learning Service. The notebook will cover the following topics:\n",
    "\n",
    "- Azure Machine Learning service workspace and basic concepts\n",
    "- Creating a dataset using Azure Cognitive Services and hosting it on a blob datastore\n",
    "- Setting up an auto-scaling AML Compute cluster\n",
    "- Training a distributed model across multiple nodes and GPUs\n",
    "- Deploying a model and creating a web service endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Professional sporting events have the power to bring friends, families, and co-workers together at home, at bars, and at the venue itself. While most people enjoy the social aspect and the entertainment of the game, few can recognize every single athlete that is on the court, field, or ice.\n",
    "\n",
    "In this demo, we will be building a model that help solves this problem by classifying NBA players based upon their photo. While this can be extended to a multi-classification problem, this example will focus on a binary classification between two of the top NBA players today - Lebron James and Stephen Curry.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <h2>Lebron James</h2>\n",
    "            <img src='./test/lebron.jpg' height='200' width='400'>\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2>Stephen Curry</h2>\n",
    "            <img src='./test/steph.jpg' height='300' width='475'>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning Service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](./images/aml-overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "The tutorial requires the following prerequisites:\n",
    "\n",
    "- An existing Azure Machine Learning service workspace\n",
    "- An existing notebook VM instance inside the workspace\n",
    "\n",
    "The tutorial is designed to be run on the notebook VM instance. Click [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup) for instructions on creating a workspace and notebook VM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. The workspace holds all your experiments, compute targets, models, datastores, etc.\n",
    "\n",
    "You can [open ml.azure.com](https://ml.azure.com) to access your workspace resources through a graphical user interface of **Azure Machine Learning studio**.\n",
    "\n",
    "![](./images/aml-workspace.png)\n",
    "\n",
    "**You will be asked to login in the next step. Use your Microsoft AAD credentials.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Using Azure Cognitive Services\n",
    "\n",
    "[Azure Cognitive Services](https://azure.microsoft.com/en-us/services/cognitive-services/) is a comprehensive family of AI services and cognitive APIs. We will be using the [Bing Image Search API](https://azure.microsoft.com/en-us/services/cognitive-services/bing-image-search-api/) to find and download images of professional athletes in order to build a dataset that can be used for training.\n",
    "\n",
    "![](./images/cog-services.png)\n",
    "\n",
    "To complete this section, you will need to create an Azure Cognitive Service resource in order to obtain an API key. [Click here](https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account?tabs=multiservice%2Cwindows) for instructions on how to do so.\n",
    "\n",
    "**Save the API key in a variable that we will use later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cognitive_service_api_key = '<API-KEY-HERE>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have prepared a script which does the following steps in sequence:\n",
    "\n",
    "1. For each athlete, make a query to the Bing Image Search API for profile photos.\n",
    "2. Download each image from the resulting response and crop out the face of the athlete.\n",
    "3. Save the images to a data directory in corresponding test/valid and label folders.\n",
    "\n",
    "**Let's take a look at the data preparation script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat src/cognitive-service-web-scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install the necessary packages to the run the script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opencv-python imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script downloads images in a predefined directory. In order to make it easier for training later on, we will mount a [Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) container into our local directory and download the images directly into that datastore.\n",
    "\n",
    "When an Azure Machine Learning workspace is created, a datastore is created by default.\n",
    "\n",
    "**Get the default datastore of the workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default datastore\n",
    "datastore = workspace.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a blobfuse config file which is required for mounting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_name = 'blobfuse_connection.cfg'\n",
    "\n",
    "config_params = 'accountName {}\\naccountKey {}\\ncontainerName {}'.format(\n",
    "    datastore.account_name, \n",
    "    datastore.account_key, \n",
    "    datastore.container_name)\n",
    "\n",
    "config_file = open(config_file_name, 'w')\n",
    "config_file.writelines(config_params)\n",
    "config_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create directory that will map to the Blob Storage container**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobstore_dir = 'blobstore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(blobstore_dir):\n",
    "    os.makedirs(blobstore_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mount the datastore using blobfuse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo blobfuse {blobstore_dir} --tmp-path=/mnt/resource/blobfusetmp  --config-file={config_file_name} -o attr_timeout=240 -o entry_timeout=240 -o negative_timeout=120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can run the script to build our dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'nba-players'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mkdir {blobstore_dir}/{data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!sudo {sys.executable} cognitive-service-web-scraper.py --root_dir {blobstore_dir}/{data_dir} --image_dim 400 --api_key {cognitive_service_api_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset\n",
    "\n",
    "Now we can register the directory in our datastore as a [Dataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py), which is simply a reference to a file or folder in a storage container. Registered datasets can then be mounted to compute targets during training making it very easy to have your data be accessible on any machine during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "nba_players_dataset = Dataset.File.from_files(path=(datastore, data_dir))\n",
    "\n",
    "nba_players_dataset = nba_players_dataset.register(workspace=workspace,\n",
    "                                       name='NBA Players Dataset',\n",
    "                                       description='Dataset containing pictures of NBA players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the dataset has already been registered, then you (and other users in your workspace) can directly run this cell to get access to it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_players_dataset = workspace.datasets['NBA Players Dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Compute Target\n",
    "\n",
    "A [compute target](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py) is a designated compute resource/environment where you run your training script or host your service deployment. This location may be your local machine or a cloud-based compute resource. Compute targets can be reused across the workspace for different runs and experiments. \n",
    "\n",
    "For this tutorial, we will create an auto-scaling [Azure Machine Learning Compute](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute?view=azure-ml-py) cluster, which is a managed-compute infrastructure that allows the user to easily create a single or multi-node compute. To create the cluster, we need to specify the following parameters:\n",
    "\n",
    "- `vm_size`: The is the type of GPUs that we want to use in our cluster. For this tutorial, we will use **Standard_NC12 (NVIDIA K80) GPU Machines** .\n",
    "- `idle_seconds_before_scaledown`: This is the number of seconds before a node will scale down in our auto-scaling cluster. We will set this to **6000** seconds. \n",
    "- `min_nodes`: This is the minimum numbers of nodes that the cluster will have. To avoid paying for compute while they are not being used, we will set this to **0** nodes.\n",
    "- `max_modes`: This is the maximum number of nodes that the cluster will scale up to. Will will set this to **4** nodes.\n",
    "\n",
    "**When jobs are submitted to the cluster it takes approximately 5 minutes to allocate new nodes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "cluster_name = 'k80cluster'\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',\n",
    "                                                       idle_seconds_before_scaledown=1200,\n",
    "                                                       min_nodes=0, \n",
    "                                                       max_nodes=2)\n",
    "\n",
    "compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the compute target has already been created, then you (and other users in your workspace) can directly run this cell to get access to it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target = workspace.compute_targets['k80cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Experiment\n",
    "\n",
    "Now that we have our dataset registered and compute target created, it is time to start training our model. We will start by creating an [experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py). An experiment is a grouping of many runs from a specified script. All runs in this tutorial will be performed under the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'nba-player-classifier' \n",
    "experiment = Experiment(workspace, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training script has been prepared in advance and will be used to train our model. The Tensorflow 2.0 training script adds dropout and classification layers to a base ResNet50 model and then fine-tunes the weights on our new dataset.\n",
    "\n",
    "**Let's take a look at what the script looks like exactly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat src/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next step is to create a TensorFlow Estimator**\n",
    "\n",
    "The Azure Machine Learning Python SDK Estimator classes allow you to easily construct run configurations for your experiments. They allow you too define parameters such as the training script to run, the compute target to run it on, framework versions, additional package requirements, etc. \n",
    "\n",
    "You can also use a generic [Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py) to submit training scripts that use any learning framework you choose.\n",
    "\n",
    "For popular libaries like PyTorch and Tensorflow you can use their framework specific estimators. We will use the [TensorFlow Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py) for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(source_directory='src',\n",
    "                                  entry_script='train.py',\n",
    "                                  compute_target=compute_target,\n",
    "                                  script_params = {\n",
    "                                      '--data_dir': nba_players_dataset.as_named_input('nbaplayersdata').as_mount(),\n",
    "                                      '--image_dim': 250,\n",
    "                                      '--learning_rate': 0.001,\n",
    "                                      '--batch_size': 16,\n",
    "                                      '--steps_per_epoch': 100,\n",
    "                                      '--num_epochs': 25,\n",
    "                                      '--dropout_rate': 0.5,\n",
    "                                      '--activation_function': 'softmax',\n",
    "                                      '--output_dir':'./outputs',\n",
    "                                  },\n",
    "                                  framework_version='2.0',\n",
    "                                  use_gpu=True,\n",
    "                                  pip_packages=['Pillow==6.2.0', 'scipy==1.1.0', 'azureml-dataprep[fuse,pandas]==1.1.29'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we submit our estimator to the experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can monitor the progress of our run within the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can register the trained model into our workspace**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='nba-player-classifier', \n",
    "                           model_path='outputs',\n",
    "                           datasets=[('train, test, validation data', nba_players_dataset)],\n",
    "                           description='ResNet 50 model of for classifying NBA players')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are satisified with the performance of our model, we can deploy it as a web service. For this tutorial, we will be deploying our service to an [Azure Container Instance (ACI)](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.aciwebservice?view=azure-ml-py). Azure Machine Learning has options for [local](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.localwebservice?view=azure-ml-py) and [Azure Kubernetes Service (AKS)](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.akswebservice?view=azure-ml-py) deployment as well. Deployment of a model requires the following dependencies:\n",
    "\n",
    "- A scoring script to show how to use the model\n",
    "- An environment file to show what packages need to be installed\n",
    "- A configuration file to build the web service\n",
    "- The model that was previously trained\n",
    "\n",
    "The scoring script requires two methods:\n",
    "\n",
    "- **init()**: This method is run upon deployment of the web service container. This is typically where the model is loaded.\n",
    "- **run()**: This method is run each time a request is sent to the web service. This is your predict method.\n",
    "\n",
    "**Let's take a look at the scoring script that has already been created.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat src/score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assist with creating the environment file, we will be using the [CondaDepencies](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py) class to define the required python packages for running our scoring script. \n",
    "\n",
    "**The code below creates the environment file that we need for deployment.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','pandas'],\n",
    "                                 pip_packages=['inference-schema[numpy-support]', 'azureml-defaults', 'tensorflow==2.0.0', 'opencv-python==4.1.1.26'])\n",
    "\n",
    "with open('src/myenv.yml','w') as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's take a look at the environment file that was created**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat src/myenv.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We define our deployment configuration using the [InferenceConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py) class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(source_directory='src',\n",
    "                                   runtime= 'python', \n",
    "                                   entry_script='score.py',\n",
    "                                   conda_file='myenv.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can create and deploy our web service (deployment will take about 10 minutes).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "model = Model(workspace, name='nba-player-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores=2, \n",
    "                                                memory_gb=4, \n",
    "                                                tags={'model': 'RESNET 50', 'method': 'tensorflow'}, \n",
    "                                                description='Predicts NBA player based on image')\n",
    "\n",
    "aci_service_name = 'nba-player-aciservice'\n",
    "\n",
    "try:\n",
    "    aci_service = Webservice(workspace, name=aci_service_name)\n",
    "    if aci_service:\n",
    "        aci_service.delete()\n",
    "except WebserviceException as e:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aci_service = Model.deploy(workspace, aci_service_name, [model], inference_config, aci_config)\n",
    "\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Web Service\n",
    "\n",
    "Once the model has been successfully deployed, we can test the web service.\n",
    "\n",
    "**We can easily check out the HTTP endpoint of the web service.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import Webservice\n",
    "aci_service = Webservice(workspace, name='nba-player-aciservice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Send in some requests to test our service.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "test_images = [\n",
    "    'https://a.espncdn.com/photo/2019/0603/r551314_1296x729_16-9.jpg', \n",
    "    'https://media.voltron.voanews.com/Drupal/01live-166/2019-04/00F04247-6B94-4259-AE2E-916E333E9C79.jpg'\n",
    "]\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for url in test_images:\n",
    "    input_data = '{\\\"image_url\\\": \\\"'+ url +'\\\"}'\n",
    "    headers = {'Content-Type':'application/json'}\n",
    "    response = requests.post(aci_service.scoring_uri, input_data, headers=headers)\n",
    "    predictions.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "labels = ['Lebron James', 'Stephen Curry']\n",
    "\n",
    "# Download images\n",
    "img_A = Image.open(BytesIO(requests.get(test_images[0]).content))\n",
    "img_B = Image.open(BytesIO(requests.get(test_images[1]).content))\n",
    "\n",
    "# Get predictions\n",
    "result_A = np.array(json.loads(json.loads(predictions[0].text))['prediction'][0])\n",
    "result_B = np.array(json.loads(json.loads(predictions[1].text))['prediction'][0])\n",
    "prediction_A = 'Prediction: {}\\n Probability: {}'.format(labels[result_A.argmax()], result_A.max())\n",
    "prediction_B = 'Prediction: {}\\n Probability: {}'.format(labels[result_B.argmax()], result_B.max())\n",
    "\n",
    "# Display images\n",
    "rcParams['figure.figsize'] = 20,15\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].set_title(prediction_A)\n",
    "ax[0].imshow(img_A)\n",
    "ax[1].set_title(prediction_B)\n",
    "ax[1].imshow(img_B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
